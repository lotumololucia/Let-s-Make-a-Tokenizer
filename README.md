# Let-s-Make-a-Tokenizer

Este proyecto implementa un tokenizer (tokenizador) básico escrito en Python, que utiliza expresiones regulares (regex) y una pequeña gramática para dividir texto en unidades lingüísticas llamadas tokens.

Fue realizado como parte de mi aprendizaje en Procesamiento de Lenguaje Natural (NLP).
El código original pertenece a Daniel J. Dorado — Computational Linguist, y este repositorio fue creado con fines educativos para entender su funcionamiento interno, modificarlo y documentar cada parte del proceso.
